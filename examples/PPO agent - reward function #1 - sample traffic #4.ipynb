{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49885085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "from lib.envs.slicing_env import SlicingEnvironment\n",
    "from lib.agents.tforce import TensorforceAgent\n",
    "from tensorforce import Environment, Agent\n",
    "from lib import utils\n",
    "from scipy.stats import poisson\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719fc5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the simulation\n",
    "np.random.seed(2021)\n",
    "\n",
    "# Number of DRL agent timesteps per episode \n",
    "max_episode_timesteps = 2\n",
    "\n",
    "total_data_episodes = 1\n",
    "\n",
    "# Number of DRL agent episodes (we are doing episodes just for the sake of better results visulization)\n",
    "\n",
    "total_episodes = 50000\n",
    "\n",
    "\n",
    "# number of users per slice in the following order: VoLTE, Video, URLLC\n",
    "num_users = [int(46/4), int(46/4), int(8/4)]\n",
    "\n",
    "poisson_volte = np.full((1, 200), 1)\n",
    "poisson_video = np.full((1, 200), 1)\n",
    "poisson_urllc = np.full((1, 200), 1)\n",
    "\n",
    "max_num_users = [max(poisson_volte[0]), max(poisson_video[0]), max(poisson_urllc[0])]\n",
    "\n",
    "num_users_poisson = [poisson_video[0], poisson_volte[0], poisson_urllc[0]]\n",
    "\n",
    "\n",
    "agent_name = 'ppo'\n",
    "learning_type = 'non_accelerated'\n",
    "\n",
    "max_traffic_percentage = 1\n",
    "num_action_lvls = 15\n",
    "num_slices = 3\n",
    "sl_win_size = 40\n",
    "time_quantum = 1\n",
    "\n",
    "\n",
    "max_size_per_tti = 40\n",
    "max_num_packets = 0\n",
    "\n",
    "max_trans_per_tti = 6\n",
    "\n",
    "c1_volte = 0.5\n",
    "c2_volte = 10\n",
    "c1_urllc = 2\n",
    "c2_urllc = 3\n",
    "c1_video = 1\n",
    "c2_video = 7\n",
    "\n",
    "num_traffic_var = 2\n",
    "\n",
    "discount_factor=0.5\n",
    "epsilon=0.9\n",
    "epsilon_decay=0.99\n",
    "decay_steps=16000\n",
    "loaded_qtable='no'\n",
    "batch_size = 4\n",
    "memory = 20000\n",
    "device='CPU'\n",
    "\n",
    "random_seed = 2021 \n",
    "learning_rate = 0.001\n",
    "final_value = 0.01\n",
    "epsilon_every_x_steps = 50\n",
    "max_epsilon_steps = 18000\n",
    "reward_function_type = 'simple'\n",
    "traffic_pattern = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df = utils.generate_data(max_num_users[0], max_num_users[1], \n",
    "                                 max_num_users[2], sl_win_size*max_episode_timesteps, traffic_pattern)\n",
    "traffic_df = traffic_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651fb2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df[traffic_df['type'] == 'volte'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d418512",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_volte = 0.1\n",
    "w_urllc = 0.7\n",
    "w_video = 0.2\n",
    "\n",
    "enviro = SlicingEnvironment(traffic_df, max_num_packets, max_size_per_tti, num_action_lvls, \n",
    "                     num_slices, max_episode_timesteps, sl_win_size, time_quantum,total_data_episodes,\n",
    "                     num_users_poisson, max_traffic_percentage, max_trans_per_tti, w_volte, w_urllc,\n",
    "                        w_video, c1_volte, c1_urllc, c1_video, c2_volte, c2_urllc, c2_video, num_traffic_var,\n",
    "                           reward_function_type)\n",
    "\n",
    "\n",
    "environment = Environment.create(\n",
    "        environment=enviro, max_episode_timesteps=max_episode_timesteps\n",
    "    )\n",
    "\n",
    "\n",
    "slicing_agent = TensorforceAgent(agent_name, environment, batch_size , memory, epsilon, epsilon_decay,\n",
    "                                loaded_qtable, decay_steps, device, learning_rate, final_value, \n",
    "                                random_seed, discount_factor)\n",
    "\n",
    "# choose algorithm and create tensorforce agent\n",
    "ep_rewards = {}\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa809ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "slicing_agent.agent.config.seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011fca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(total_episodes):\n",
    "    if (episode >= decay_steps ):\n",
    "        epsilon = final_value\n",
    "    if (episode >= max_epsilon_steps ):\n",
    "        epsilon = 0\n",
    "    # Record episode experience\n",
    "    episode_states = list()\n",
    "    episode_internals = list()\n",
    "    episode_actions = list()\n",
    "    episode_terminal = list()\n",
    "    episode_reward = list()\n",
    "    states = environment.reset()\n",
    "\n",
    "    # Episode using independent-act and agent.intial_internals()\n",
    "    internals = slicing_agent.agent.initial_internals()\n",
    "    terminal = False\n",
    "    sum_rewards = 0.0\n",
    "    \n",
    "    while not terminal:\n",
    "        print('Episode: ' + str(episode) + ', Step: ' + str(step))\n",
    "        print(\"---current observation: \", states)\n",
    "        print(\"---agent algorithm: \", slicing_agent.agent_algorithm)\n",
    "        episode_states.append(states)\n",
    "        episode_internals.append(internals)\n",
    "        actions, internals = slicing_agent.agent.act(states=states, internals=internals, independent=True,\\\n",
    "                                                    deterministic=False)\n",
    "\n",
    "        # epsilon greedy\n",
    "        print('epsilon is: ', epsilon)\n",
    "        p = np.random.random()\n",
    "        if p < epsilon:\n",
    "            actions = np.random.choice(num_action_lvls)\n",
    "\n",
    "        if ( (0 < episode < decay_steps) and (episode%epsilon_every_x_steps==0) and (epsilon>final_value)):\n",
    "            epsilon = epsilon * epsilon_decay\n",
    "\n",
    "        print(\"---agent action: \", actions)\n",
    "        episode_actions.append(actions)\n",
    "        states, terminal, reward = environment.execute(actions=actions)\n",
    "        print(\"---reward, done: \", reward, terminal)\n",
    "        episode_terminal.append(terminal)\n",
    "        episode_reward.append(reward)\n",
    "        sum_rewards += reward\n",
    "        print ('cumulative episode reward: ', sum_rewards)\n",
    "        step += 1\n",
    "\n",
    "    print('Episode {}: {}'.format(episode, sum_rewards))\n",
    "    ep_rewards[episode] = sum_rewards\n",
    "    print(\"End episode: \", episode)\n",
    "    print ('episode total reward: ', ep_rewards[episode])\n",
    "\n",
    "    # Feed recorded experience to agent\n",
    "    slicing_agent.agent.experience(\n",
    "        states=episode_states, internals=episode_internals, actions=episode_actions,\n",
    "        terminal=episode_terminal, reward=episode_reward\n",
    "    )\n",
    "\n",
    "    # Perform update\n",
    "    slicing_agent.agent.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7b66d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_list = list(ep_rewards.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e1c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "slicing_rewards = [sum(reward_list[i:i+1000]) for i in range(0, len(reward_list), 1000)]\n",
    "slicing_rewards_temp = slicing_rewards.copy()\n",
    "slicing_rewards_temp.extend([slicing_rewards_temp[-1]]*400)\n",
    "temp_np_slicing = np.array(slicing_rewards_temp)/max(slicing_rewards_temp)\n",
    "normalized_np_slicing = temp_np_slicing*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fcdd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "plt.style.use('seaborn')\n",
    "plt.title('Learning Performance (' + str(slicing_agent.agent_algorithm) + ' agent)')\n",
    "plt.plot(np.arange(1,len(normalized_np_slicing)+1)[0:20]*1000, normalized_np_slicing[0:20], label='Reward', marker=\"\", linestyle=\"-\")#, color='k')\n",
    "plt.xlabel('Learning Step')\n",
    "plt.ylabel('Rewards')\n",
    "plt.legend(prop={'size': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48c2293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a65480e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfceedd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
